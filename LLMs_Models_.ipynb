{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VlX3wx-kOd5o",
        "1sGJRcQMAhVv"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/code-deenah/AI-Physics-Tutor/blob/main/LLMs_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dolly 3B Model - by Databricks\n",
        "---\n",
        "1. Model Name: Dolly-v2-3b\n",
        "2. Model Parameters: 3 Billion\n",
        "3. Training: Instruction-tuned Model\n",
        "4. Link: https://huggingface.co/databricks/dolly-v2-3b\n",
        "---"
      ],
      "metadata": {
        "id": "VlX3wx-kOd5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08FhM_McPQo_",
        "outputId": "e95fd863-fb97-4295-a65a-d4cbd1ba2243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                        # allows Tensor computation with strong GPU acceleration\n",
        "from transformers import pipeline   # fast way to use pre-trained models for inference\n",
        "import os"
      ],
      "metadata": {
        "id": "V8yC4f7bSZ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "dolly_pipeline = pipeline(model=\"databricks/dolly-v2-3b\",\n",
        "                            torch_dtype=torch.bfloat16,\n",
        "                            trust_remote_code=True,\n",
        "                            device_map=\"auto\")"
      ],
      "metadata": {
        "id": "TiKmR1Y4QC-q",
        "outputId": "1e262605-d6d6-40ed-abbd-0314fa8b0c11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define helper function\n",
        "def get_completion_dolly(input):\n",
        "  system = f\"\"\"\n",
        "  You are an expert Physicist.\n",
        "  You are good at explaining Physics concepts in simple words.\n",
        "  Help as much as you can.\n",
        "  \"\"\"\n",
        "  prompt = f\"#### System: {system}\\n#### User: \\n{input}\\n\\n#### Response from Dolly-v2-3b:\"\n",
        "  print(prompt)\n",
        "  dolly_response = dolly_pipeline(prompt,\n",
        "                                  max_new_tokens=500\n",
        "                                  )\n",
        "  return dolly_response[0][\"generated_text\"]"
      ],
      "metadata": {
        "id": "0WQAeTibSmkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's prompt\n",
        "prompt = \"Explain the difference between nuclear fission and fusion.\"\n",
        "# prompt = \"Why is the Sky blue?\"\n",
        "\n",
        "print(get_completion_dolly(prompt))"
      ],
      "metadata": {
        "id": "0n_AAFyejwdh",
        "outputId": "e383811e-c899-4e9c-8197-4ead1b76eab5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#### System: \n",
            "  You are an expert Physicist.\n",
            "  You are good at explaining Physics concepts in simple words.\n",
            "  Help as much as you can.\n",
            "  \n",
            "#### User: \n",
            "Explain the difference between nuclear fission and fusion.\n",
            "\n",
            "#### Response from Dolly-v2-3b:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build AI Physics Tutor"
      ],
      "metadata": {
        "id": "eylMkhfTogUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "xGtgWf5PpvXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "# print(locale.getpreferredencoding())\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working"
      ],
      "metadata": {
        "id": "Gs5hjtiPUTQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# build an app front-end / ai tool with Gradio\n",
        "iface = gr.Interface(fn=get_completion_dolly, inputs=[gr.Textbox(label=\"Insert Prompt Here\",lines=6)],\n",
        "                     outputs=[gr.Textbox(label=\"Your Answer Here\",lines=3)],\n",
        "                     title=\"My AI Physics Teacher\",\n",
        "                     examples=[\"Explain the difference between nuclear fusion and fission.\",\n",
        "                              \"Why is the Sky blue?\"]\n",
        "                     )\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "id": "jq9UlgTfidll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.close()"
      ],
      "metadata": {
        "id": "K57QraCUV88d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Falcon 7B Model - by TII\n",
        "---\n",
        "1. Model Name: Falcon-7b-instruct\n",
        "2. Model Parameters: 7 Billion\n",
        "3. Training: Instruction-tuned Model\n",
        "4. Link: https://huggingface.co/tiiuae/falcon-7b-instruct\n",
        "---"
      ],
      "metadata": {
        "id": "1sGJRcQMAhVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install transformers\n",
        "!pip install einops\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "lL-uD6XfAtZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# load model\n",
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "falcon_pipeline = transformers.pipeline(\"text-generation\",\n",
        "                                        model=model,\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        torch_dtype=torch.bfloat16,\n",
        "                                        trust_remote_code=True,\n",
        "                                        device_map=\"auto\"\n",
        "                                        )"
      ],
      "metadata": {
        "id": "YLeSpPwfAmtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define completion function\n",
        "def get_completion_falcon(input):\n",
        "  # prompt = f\"You are an expert Physicist. Your job is to explain complex Physics concepts in simple words. \\n{input}\"\n",
        "  # print(prompt)\n",
        "  system = f\"\"\"\n",
        "  You are an expert Physicist.\n",
        "  You are good at explaining Physics concepts in simple words.\n",
        "  Help as much as you can.\n",
        "  \"\"\"\n",
        "  prompt = f\"#### System: {system}\\n#### User: \\n{input}\\n\\n#### Response from falcon-7b-instruct:\"\n",
        "  print(prompt)\n",
        "  falcon_response = falcon_pipeline(prompt,\n",
        "                                    max_length=500,\n",
        "                                    do_sample=True,\n",
        "                                    top_k=10,\n",
        "                                    num_return_sequences=1,\n",
        "                                    eos_token_id=tokenizer.eos_token_id,\n",
        "                                    )\n",
        "\n",
        "  return falcon_response"
      ],
      "metadata": {
        "id": "0HkOj80TqaU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's prompt\n",
        "prompt = \"Explain to me the difference between nuclear fission and fusion.\"\n",
        "# prompt = \"Why is the Sky blue?\"\n",
        "response = get_completion_falcon(prompt)\n",
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "8DRr9QcmuBVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0]['generated_text'])"
      ],
      "metadata": {
        "id": "jEVqNekqZMyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion v1-5 - by RunwayML\n",
        "---\n",
        "1. Model Name: stable-diffusion-v1-5\n",
        "2. Model type: Diffusion-based text-to-image generation model\n",
        "3. Model Description: Generate/ modify images w/ text prompts.\n",
        "4. Link: https://huggingface.co/runwayml/stable-diffusion-v1-5\n",
        "---"
      ],
      "metadata": {
        "id": "Byov1aYybtaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Developed by: Robin Rombach, Patrick Esser\n",
        "# Latent Diffusion Model that uses a fixed, pretrained text encoder (CLIP ViT-L/14) as suggested in the Imagen paper.\n",
        "# Language(s): English\n",
        "# License: The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that\n",
        "# BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing.\n",
        "# See also the article about the BLOOM Open RAIL license on which our license is based.\n",
        "# https://bigscience.huggingface.co/blog/the-bigscience-rail-license"
      ],
      "metadata": {
        "id": "XAlPILTbKqYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install diffusers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "DWFtAd5Yb5IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "sd_pipeline = StableDiffusionPipeline.from_pretrained(model_id,\n",
        "                                                      torch_dtype=torch.float16)\n",
        "sd_pipeline = sd_pipeline.to(\"cuda\")"
      ],
      "metadata": {
        "id": "kzmeuw5vT7qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_sd(prompt):\n",
        "  negative_prompt = \"\"\"\n",
        "  simple background, duplicate, low quality, lowest quality,\n",
        "  bad anatomy, bad proportions, extra digits, lowres, username,\n",
        "  artist name, error, duplicate, watermark, signature, text,\n",
        "  extra digit, fewer digits, worst quality, jpeg artifacts, blurry\n",
        "  \"\"\"\n",
        "  return sd_pipeline(prompt, negative_prompt=negative_prompt).images[0]"
      ],
      "metadata": {
        "id": "1Rawmfx_jLpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's prompt\n",
        "\n",
        "# prompt = \"astronaut, riding a horse, on mars, human colony\"\n",
        "# prompt = \"children, playing in disneyland, view from a distance\"\n",
        "prompt = \"\"\"baby llama, wearing red muffler,\n",
        "grazing, open field, sunset\n",
        "\"\"\"\n",
        "print(prompt)\n",
        "sd_image = get_completion_sd(prompt)\n",
        "sd_image.save(\"./llama.jpg\")"
      ],
      "metadata": {
        "id": "Olq24pYGUIpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build AI Image Generator"
      ],
      "metadata": {
        "id": "7ehwKQRXX-pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio==3.48.0"
      ],
      "metadata": {
        "id": "NMm4rKmyX-pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "oAE-vWuKX618"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt):\n",
        "  negative_prompt = \"\"\"\n",
        "  simple background, duplicate, low quality, lowest quality,\n",
        "  bad anatomy, bad proportions, extra digits, lowres, username,\n",
        "  artist name, error, duplicate, watermark, signature, text,\n",
        "  extra digit, fewer digits, worst quality, jpeg artifacts, blurry\n",
        "  \"\"\"\n",
        "  return sd_pipeline(prompt, negative_prompt=negative_prompt).images[0]\n",
        "\n",
        "# def generate(prompt):\n",
        "#     output = get_completion_sd(prompt)\n",
        "#     return output\n",
        "\n",
        "genai_app = gr.Interface(fn=get_completion,\n",
        "                         inputs=[gr.Textbox(label=\"Your prompt\")],\n",
        "                         outputs=[gr.Image(label=\"Result\")],\n",
        "                         title=\"Generate Cool Images\",\n",
        "                         description=\"Generate any image with Stable Diffusion\",\n",
        "                         allow_flagging=\"never\",\n",
        "                         examples=[\"astronaut, riding a horse, on mars\",\n",
        "                                   \"cargo ship, flying, in space\"])\n",
        "genai_app.launch()"
      ],
      "metadata": {
        "id": "u-4e6fGXc3Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gr.close_all()"
      ],
      "metadata": {
        "id": "tvDTwKTjrfB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}